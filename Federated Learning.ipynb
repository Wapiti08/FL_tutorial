{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import tensorflow as tf\r\n",
    "\r\n",
    "tf.compat.v1.enable_v2_behavior()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import tensorflow_datasets as tfds\r\n",
    "\r\n",
    "# # load datasets\r\n",
    "alice_dataset, bob_dataset = tfds.load(\"fashion_mnist\", split=[\"train[:50]\",\"train[-50:]\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ren\\tensorflow_datasets\\fashion_mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:02,  1.03 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:01<00:02,  1.03 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:01<00:02,  1.03 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.22 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:05<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:06<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:11<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:11<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:16<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:17<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:21<00:01,  1.22 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:24<00:01,  1.22 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:26<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:26<00:08,  8.09s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:26<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:27<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:33<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:38<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:43<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:48<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:53<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:58<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:02<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:08<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:15<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:20<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:26<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:31<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:36<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:41<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:45<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:50<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [01:55<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [02:01<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [02:06<00:08,  8.09s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [02:10<00:08,  8.09s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [02:11<00:00, 37.22s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [02:11<00:00, 37.22s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [02:12<00:00, 37.22s/ url]\n",
      "Extraction completed...: 100%|██████████| 4/4 [02:12<00:00, 33.08s/ file]\n",
      "Dl Size...: 100%|██████████| 29/29 [02:12<00:00,  4.56s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 4/4 [02:12<00:00, 33.08s/ url]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mDataset fashion_mnist downloaded and prepared to C:\\Users\\ren\\tensorflow_datasets\\fashion_mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import collections\r\n",
    "\r\n",
    "BATCH_SIZE = 32\r\n",
    "\r\n",
    "\r\n",
    "def cast(element):\r\n",
    "    \"\"\"Casts an image's pixels into float32.\"\"\"\r\n",
    "    out = {}\r\n",
    "    out[\"image\"] = tf.image.convert_image_dtype(element[\"image\"], dtype=tf.float32)\r\n",
    "    out[\"label\"] = element[\"label\"]\r\n",
    "    return out\r\n",
    "\r\n",
    "\r\n",
    "def flatten(element):\r\n",
    "    \"\"\"Flattens an image in preparation for the neural network.\"\"\"\r\n",
    "    return collections.OrderedDict(\r\n",
    "        [\r\n",
    "            (\"x\", tf.reshape(element[\"image\"], [-1])),\r\n",
    "            (\"y\", tf.reshape(element[\"label\"], [1])),\r\n",
    "        ]\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "def preprocess(dataset):\r\n",
    "    \"\"\"Preprocesses images to be fed into neural network.\"\"\"\r\n",
    "    return dataset.map(cast).map(flatten).batch(BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# batches of data\r\n",
    "preprocessed_alice_dataset = preprocess(alice_dataset)\r\n",
    "preprocessed_bob_dataset = preprocess(bob_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "federated_data = [preprocessed_alice_dataset, preprocessed_bob_dataset]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\r\n",
    "\r\n",
    "LEARNING_RATE = 0.02\r\n",
    "\r\n",
    "\r\n",
    "def custom_loss_function(y_true, y_pred):\r\n",
    "    \"\"\"Custom loss function.\"\"\"\r\n",
    "    #return tf.reduce_mean(\r\n",
    "    #    tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\r\n",
    "    #)\r\n",
    "\r\n",
    "    return tf.reduce_mean(\r\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "def create_compiled_keras_model():\r\n",
    "    \"\"\"Compiles the keras model.\"\"\"\r\n",
    "    model = tf.keras.models.Sequential(\r\n",
    "        [\r\n",
    "            tf.keras.layers.Dense(\r\n",
    "                10,\r\n",
    "                activation=tf.nn.softmax,\r\n",
    "                kernel_initializer=\"zeros\",\r\n",
    "                input_shape=(784,),\r\n",
    "            )\r\n",
    "        ]\r\n",
    "    )\r\n",
    "    model.compile(\r\n",
    "        loss=custom_loss_function,\r\n",
    "        optimizer=gradient_descent.SGD(learning_rate=LEARNING_RATE),\r\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n",
    "    )\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "batch_of_samples = tf.contrib.framework.nest.map_structure(\r\n",
    "    lambda x: x.numpy(), iter(preprocessed_alice_dataset).next()\r\n",
    ")\r\n",
    "\r\n",
    "def model_instance():\r\n",
    "    \"\"\"Instantiates the keras model.\"\"\"\r\n",
    "    keras_model = create_compiled_keras_model()\r\n",
    "    # training model by adding model and samples\r\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, batch_of_samples)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8c242273874b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m batch_of_samples = tf.contrib.framework.nest.map_structure(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_alice_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow_federated import python as tff\r\n",
    "\r\n",
    "federated_learning_iterative_process = tff.learning.build_federated_averaging_process(\r\n",
    "    model_instance\r\n",
    ")\r\n",
    "state = federated_learning_iterative_process.initialize()\r\n",
    "state, performance = federated_learning_iterative_process.next(state, federated_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "performance"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "8fecd871876602184e2def9d040398806a20c493ba8c7291bbd5a5358628e6cd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}